\documentclass{article}

% Standard packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}

% For figures
\graphicspath{{figures/}}

% Custom commands
\newcommand{\Cl}{\mathrm{Cl}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

\title{HC-Net: Scalable Geometric Deep Learning\\via Clifford Algebra Grade Hierarchy}

\author{
  Sungwoo Kang\\
  Department of Electrical and Computer Engineering\\
  Korea University, Seoul 02841, Republic of Korea\\
  \texttt{krml919@korea.ac.kr}
}

\date{}

\begin{document}

\maketitle

%==============================================================================
\begin{abstract}
%==============================================================================
We identify a strict \emph{grade hierarchy} in Clifford algebras for geometric deep learning: under mean-field aggregation, scalars and vectors lose all directional information ($\sim$50\% classification accuracy); bivectors preserve rotation direction (100\%) but not chirality ($\sim$50\%); only \textbf{trivectors} preserve chirality (100\%).
This hierarchy is not incremental---each grade captures qualitatively different geometric information that lower grades provably cannot represent.
Building on this insight, we propose HC-Net, a hybrid architecture combining $k$-nearest-neighbor message passing with $\mathcal{O}(N)$ Clifford mean-field aggregation in the full $\Cl(3,0)$ algebra.
On synthetic 3D N-body benchmarks, HC-Net achieves the lowest prediction error among five equivariant architectures (324K parameters) while maintaining $\mathcal{O}(N)$ scaling---EGNN, the $\mathcal{O}(N^2)$ baseline, runs out of memory at $N \geq 500$.
Ablation on MD17 molecular force prediction reveals that local message passing and residual connections are critical ($>$550\% MSE increase when removed), while the architecture is most efficient at 2 layers with 64 hidden dimensions.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Standard vector-based mean-field aggregation suffers from a fundamental limitation we call \emph{vector averaging collapse}: when particles in a symmetric rotating system are averaged, $\frac{1}{N}\sum_i \mathbf{v}_i \approx 0$, destroying all information about rotation direction and chirality.
This is not a failure of the aggregation scheme but of the \emph{representation}: vectors are the wrong algebraic grade for encoding angular information.

The original HC-Net~\cite{kang2026hcnet} demonstrated in 2D that Clifford algebra bivectors solve this collapse: the angular momentum $L = \sum_i (x_i v_{y,i} - y_i v_{x,i}) \neq 0$ preserves rotation direction even under averaging.
In this work, we extend HC-Net to 3D using $\Cl(3,0)$, an 8-dimensional algebra containing scalars (grade 0), vectors (grade 1), bivectors (grade 2), and trivectors (grade 3).
This extension reveals a richer structure: \textbf{bivectors detect rotation but not chirality; only trivectors detect chirality}.

We validate this grade hierarchy across three progressively challenging settings:
\begin{enumerate}[nosep]
    \item \textbf{2D spinning systems} (Section~\ref{sec:exp1}): Scalar and vector mean-fields achieve $\sim$50\% on CW/CCW classification; bivector mean-field achieves 100\%.
    \item \textbf{3D synthetic data} (Section~\ref{sec:exp2}): Across rotation, chirality, and spiral tasks, each grade fails at precisely the tasks predicted by theory.
    \item \textbf{3D N-body physics} (Section~\ref{sec:exp3}): The hierarchy holds on gravitational dynamics with real physical forces.
\end{enumerate}

Building on this insight, we propose a hybrid local-global architecture (Section~\ref{sec:method}) and evaluate it on:
\begin{enumerate}[nosep,resume]
    \item \textbf{MD17 molecular force prediction} (Section~\ref{sec:exp4}): HC-Net outperforms EGNN by $5\text{--}20\times$ in force MAE across 8 molecules.
    \item \textbf{$\mathcal{O}(N)$ scaling} (Section~\ref{sec:exp5}): EGNN runs out of memory at $N \geq 500$; HC-Net scales to $N = 5000$.
    \item \textbf{3D SOTA comparison} (Section~\ref{sec:exp6}): HC-Net matches or beats CGENN~\cite{ruhe2023clifford} with 19\% fewer parameters at small training sizes.
    \item \textbf{Ablation study} (Section~\ref{sec:exp7}): Local MPNN (+559\% MSE when removed) and residual connections (+618\%) are the most critical components; the optimal architecture uses only 2 layers with hidden dimension 64.
\end{enumerate}

%==============================================================================
\section{Background}
%==============================================================================

\subsection{Clifford Algebra $\Cl(3,0)$}

The Clifford algebra $\Cl(3,0)$~\cite{hestenes1984clifford, doran2003geometric} over $\R^3$ with Euclidean signature has dimension $2^3 = 8$, decomposed by grade:
\begin{itemize}[nosep]
    \item \textbf{Grade 0} (scalar): 1 component --- invariant quantities
    \item \textbf{Grade 1} (vectors): 3 components ($e_1, e_2, e_3$) --- directions
    \item \textbf{Grade 2} (bivectors): 3 components ($e_{12}, e_{13}, e_{23}$) --- oriented planes / angular momentum
    \item \textbf{Grade 3} (trivector/pseudoscalar): 1 component ($e_{123}$) --- oriented volume / chirality
\end{itemize}

A general multivector $M \in \Cl(3,0)$ is written:
\begin{equation}
    M = \underbrace{s}_{\text{grade 0}} + \underbrace{v_1 e_1 + v_2 e_2 + v_3 e_3}_{\text{grade 1}} + \underbrace{b_{12} e_{12} + b_{13} e_{13} + b_{23} e_{23}}_{\text{grade 2}} + \underbrace{t \, e_{123}}_{\text{grade 3}}
\end{equation}

The \emph{geometric product} of two vectors $a, b$ yields:
\begin{equation}
    ab = a \cdot b + a \wedge b = \underbrace{\sum_i a_i b_i}_{\text{scalar (grade 0)}} + \underbrace{\sum_{i<j}(a_i b_j - a_j b_i) e_{ij}}_{\text{bivector (grade 2)}}
\end{equation}

Crucially, the trivector (grade 3) requires a \emph{triple} product of three linearly independent vectors.
The pseudoscalar $e_{123}$ is invariant under SO(3) but changes sign under reflections, making it the algebraic representation of \emph{handedness}.

\subsection{Grade Hierarchy for Geometric Information}

We identify a strict hierarchy of geometric information preserved by each grade under mean-field averaging:

\begin{itemize}[nosep]
    \item \textbf{Vectors (grade 1):} Position and velocity averages. In symmetric rotating systems, $\bar{\mathbf{v}} \approx 0$---both rotation direction and chirality are lost.

    \item \textbf{Bivectors (grade 2):} Angular momentum $\mathbf{L} = \mathbf{r} \times \mathbf{v}$.
    The mean $\bar{\mathbf{L}}$ distinguishes rotating from non-rotating systems.
    However, for a left-handed vs.\ right-handed helix, $\|\bar{\mathbf{L}}\|$ is identical---chirality is lost.

    \item \textbf{Trivectors (grade 3):} Helicity $h = \langle \mathbf{v}_i \cdot \bar{\mathbf{L}} \rangle$.
    For a right-handed screw, $h > 0$; for a left-handed screw, $h < 0$.
    It is SO(3)-invariant but sign-flips under reflection---precisely the signature of chirality.
\end{itemize}

This hierarchy is \emph{qualitative}: each grade captures fundamentally different geometric information that lower grades cannot represent, regardless of classifier capacity.

\subsection{Related Work}

\textbf{Equivariant neural networks.}
EGNN~\cite{satorras2021n} achieves E(n)-equivariance through invariant message passing but requires $\mathcal{O}(N^2)$ pairwise interactions.
NequIP~\cite{batzner2022nequip} and MACE~\cite{batatia2022mace} use irreducible representations and tensor products for molecular force fields.
SE(3)-Transformers~\cite{fuchs2020se3} and Tensor Field Networks~\cite{thomas2018tensor} use spherical harmonics.
PaiNN~\cite{schutt2021equivariant} combines invariant and equivariant messages.

\textbf{Clifford algebra networks.}
CGENN~\cite{ruhe2023clifford} uses the Clifford group for exact equivariance.
Geometric Algebra Transformers~\cite{brehmer2023geometric} incorporate geometric products into attention.
L-GATr~\cite{lgatr2024} uses multivector tokens in Lorentz algebra.
Szarvas and Zhdanov~\cite{szarvas2025conditional} use Clifford algebra with mean pooling for steerable CNNs.
Our work differs by identifying the \emph{grade hierarchy} and combining it with a hybrid local-global architecture for $\mathcal{O}(N)$ scaling.

%==============================================================================
\section{Method}
\label{sec:method}
%==============================================================================

\subsection{Trivector Mean-Field}

For $N$ particles with positions $\mathbf{r}_i$ and velocities $\mathbf{v}_i$, the helicity pseudoscalar is:
\begin{equation}
    \mathbf{L}_i = \mathbf{r}_i \times \mathbf{v}_i, \quad \bar{\mathbf{L}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{L}_i, \quad h = \frac{1}{N} \sum_{i=1}^{N} \mathbf{v}_i \cdot \bar{\mathbf{L}}
    \label{eq:helicity}
\end{equation}
This quantity $h$ is SO(3)-invariant, parity-odd, and computable in $\mathcal{O}(N)$.

\subsection{Hybrid Architecture}

Our architecture combines two pathways at each layer:

\textbf{1. Local MPNN Pathway} ($\mathcal{O}(kN)$, $k$ fixed):
For each atom $i$, $k$-nearest neighbors provide messages via RBF-encoded distances:
\begin{equation}
    \mathbf{m}_i = \sum_{j \in \mathcal{N}_k(i)} \phi_{\text{msg}}(\mathbf{h}_j, \text{RBF}(\|\mathbf{r}_i - \mathbf{r}_j\|)), \quad
    \mathbf{h}_i^{\text{local}} = \mathbf{h}_i + \phi_{\text{upd}}(\mathbf{h}_i, \mathbf{m}_i)
\end{equation}

\textbf{2. Global Clifford Mean-Field} ($\mathcal{O}(N)$):
Features are projected to 8D $\Cl(3,0)$ space with outer-product interaction:
\begin{equation}
    \mathbf{p}_i = W_p \mathbf{h}_i, \quad \bar{\mathbf{q}} = W_q \left(\frac{1}{N}\sum_j \mathbf{h}_j\right), \quad
    \mathbf{h}_i^{\text{global}} = \text{LN}\left(\mathbf{h}_i + \alpha \cdot W_{\text{out}}(\mathbf{p}_i \otimes \bar{\mathbf{q}})\right)
\end{equation}

\textbf{3. Fusion and CliffordBlock:}
\begin{equation}
    \mathbf{h}_i^{\text{fused}} = \text{SiLU}(W_f [\mathbf{h}_i^{\text{local}} \| \mathbf{h}_i^{\text{global}}]), \quad
    \mathbf{h}_i' = \text{CliffordBlock}(\mathbf{h}_i^{\text{fused}})
\end{equation}
The CliffordBlock applies an MLP with residual connection followed by geometric mixing via group-wise outer products.

\begin{algorithm}[t]
\caption{HC-Net Forward Pass}
\label{alg:forward}
\begin{algorithmic}[1]
\REQUIRE Positions $\mathbf{R} \in \R^{N \times 3}$, atomic numbers $\mathbf{z} \in \mathbb{Z}^N$
\STATE $\mathbf{h} \gets [\text{Embed}(\mathbf{z}) \| W_{\text{pos}} \mathbf{R}]$ \COMMENT{Atom + position embedding}
\FOR{$\ell = 1$ to $L$}
    \STATE $\mathbf{h}^{\text{local}} \gets \text{LocalMPNN}_\ell(\mathbf{h}, \mathbf{R})$ \COMMENT{$k$-NN message passing, $\mathcal{O}(kN)$}
    \STATE $\mathbf{h}^{\text{global}} \gets \text{CliffordMF}_\ell(\mathbf{h})$ \COMMENT{$\Cl(3,0)$ mean-field, $\mathcal{O}(N)$}
    \STATE $\mathbf{h} \gets \text{CliffordBlock}_\ell(\text{Fusion}_\ell([\mathbf{h}^{\text{local}} \| \mathbf{h}^{\text{global}}]))$
\ENDFOR
\RETURN $W_{\text{out}} \mathbf{h} \in \R^{N \times 3}$ \COMMENT{Force prediction}
\end{algorithmic}
\end{algorithm}

%==============================================================================
\section{Experiments}
%==============================================================================

\subsection{Experiment 1: 2D Vector Collapse Demonstration}
\label{sec:exp1}

\textbf{Setup.}
We construct the simplest possible test of the grade hierarchy: binary classification of clockwise (CW) vs.\ counter-clockwise (CCW) spinning 2D particle systems.
Each system has $N = 5$ particles in symmetric circular orbits.
Three mean-field classifiers are compared, differing \emph{only} in the grade of their per-particle features \emph{before} averaging.
Crucially, no learned features are applied before aggregation---only explicit grade projections---so the information bottleneck is genuinely at the averaging step.
\begin{itemize}[nosep]
    \item \textbf{Scalar} ($\|\mathbf{r}\|^2$, $\|\mathbf{v}\|^2$, $\mathbf{r} \cdot \mathbf{v}$): 3 invariants per particle, averaged, then classified.
    \item \textbf{Vector} ($\mathbf{r}$, $\mathbf{v}$): Raw vectors averaged, then invariant features extracted and classified.
    \item \textbf{Bivector} ($r \wedge v = x v_y - y v_x$): Angular momentum per particle, averaged, then classified.
\end{itemize}

Training: 5000 samples, 100 epochs, 3 seeds. All classifiers have identical MLP heads.

\textbf{Results.}
\begin{table}[h]
\centering
\caption{2D CW/CCW classification accuracy (\%). Scalar and vector mean-fields fail because they cannot represent rotation direction; bivector (angular momentum) preserves it perfectly.}
\label{tab:exp1}
\begin{tabular}{lccl}
\toprule
\textbf{Mean-Field Grade} & \textbf{Features} & \textbf{Accuracy} & \textbf{Interpretation} \\
\midrule
Scalar (grade 0) & 3 & 54.7 $\pm$ 0.9\% & Magnitudes carry no direction \\
Vector (grade 1) & 3 & 50.8 $\pm$ 0.2\% & $\bar{\mathbf{v}} \approx 0$ (symmetric cancellation) \\
\textbf{Bivector (grade 2)} & \textbf{1} & \textbf{100.0 $\pm$ 0.0\%} & $\bar{L} \neq 0$ (angular momentum preserved) \\
\bottomrule
\end{tabular}
\end{table}

This confirms the vector averaging collapse: with only \emph{one} feature (angular momentum), the bivector classifier achieves perfect accuracy, while the vector classifier with 3 features per particle achieves exactly chance level.

\subsection{Experiment 2: 3D Grade Hierarchy}
\label{sec:exp2}

\textbf{Setup.}
We extend the hierarchy to 3D $\Cl(3,0)$ with two tasks:
\begin{itemize}[nosep]
    \item \textbf{Rotation:} Classify CW vs.\ CCW orbiting 3D particle systems. Requires bivectors (angular momentum direction).
    \item \textbf{Chirality:} Classify left-handed vs.\ right-handed 3D helical systems. Requires trivectors (helicity).
\end{itemize}

Five mean-field representations are compared: Vector (grade 0+1), Bivector (grade 2), Trivector (grade 3), Full Clifford (all grades via geometric product), and Learned (16D projection).
Training: 5000 samples, 100 epochs, 3 seeds.

\textbf{Results.}
\begin{table}[h]
\centering
\caption{3D grade hierarchy: mean-field classification accuracy (\%). Each grade captures qualitatively different geometric information. Trivectors (1D) are the minimal representation for chirality.}
\label{tab:grade_hierarchy}
\begin{tabular}{lccccc}
\toprule
\textbf{Representation} & \textbf{Dim} & \textbf{Rotation} & \textbf{Chirality} & \textbf{Spiral} \\
\midrule
Vector (grade 0+1) & 6D & 51.9 $\pm$ 0.7\% & 52.7 $\pm$ 0.2\% & 51.3 $\pm$ 1.6\% \\
Bivector (grade 2) & 3D & \textbf{100.0 $\pm$ 0.0\%} & 52.5 $\pm$ 1.6\% & 52.1 $\pm$ 1.0\% \\
\textbf{Trivector (grade 3)} & \textbf{1D} & 50.7 $\pm$ 0.3\% & \textbf{100.0 $\pm$ 0.0\%} & \textbf{100.0 $\pm$ 0.0\%} \\
Full Clifford & 8D & 51.3 $\pm$ 0.1\% & \textbf{100.0 $\pm$ 0.0\%} & \textbf{100.0 $\pm$ 0.0\%} \\
Learned & 16D & \textbf{100.0 $\pm$ 0.0\%} & 100.0 $\pm$ 0.1\% & \textbf{100.0 $\pm$ 0.0\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}
\begin{enumerate}[nosep]
    \item \textbf{Vectors fail at all tasks} ($\sim$50\%), confirming that vector averaging collapse extends to 3D.
    \item \textbf{Bivectors detect rotation but not chirality.} Angular momentum norm $\|\bar{\mathbf{L}}\|$ distinguishes rotating from non-rotating but is identical for left- and right-handed structures.
    \item \textbf{Trivectors (1D) detect chirality but not rotation.} Helicity captures handedness but not angular momentum direction. The converse failure is equally important: it confirms each grade is \emph{specialized}.
    \item \textbf{Full Clifford succeeds at chirality but fails at rotation} (51.3\%). When all 8 grades are mixed, the bivector rotation signal is ``diluted.'' The 1D trivector is more informative per dimension than the 8D full multivector.
\end{enumerate}

\subsection{Experiment 3: Physical N-Body Validation}
\label{sec:exp3}

\textbf{Setup.}
We validate the grade hierarchy on a physical 3D N-body system (8 particles, gravitational dynamics) with chirality and rotation labels.
Seven models are compared: the five mean-field classifiers plus the full HC-Net and a variant without trivector features.

\textbf{Results.}
\begin{table}[h]
\centering
\caption{3D N-body classification accuracy (\%) and model complexity. Mean-field classifiers confirm the grade hierarchy; the full hybrid architecture achieves 100\% on both tasks.}
\label{tab:nbody}
\begin{tabular}{lccr}
\toprule
\textbf{Model} & \textbf{Chirality} & \textbf{Rotation} & \textbf{Params} \\
\midrule
Vector MF & 54.0 $\pm$ 0.2\% & 57.9 $\pm$ 3.0\% & 4,738 \\
Bivector MF & 52.7 $\pm$ 0.8\% & \textbf{100.0 $\pm$ 0.0\%} & 4,546 \\
\textbf{Trivector MF} & \textbf{100.0 $\pm$ 0.0\%} & 51.3 $\pm$ 0.7\% & 4,418 \\
Full Clifford MF & \textbf{100.0 $\pm$ 0.0\%} & 51.5 $\pm$ 0.6\% & 4,866 \\
\midrule
CliffordNet 3D & \textbf{100.0 $\pm$ 0.0\%} & \textbf{100.0 $\pm$ 0.0\%} & 324,992 \\
Hybrid HC-Net & \textbf{100.0 $\pm$ 0.0\%} & \textbf{100.0 $\pm$ 0.0\%} & 654,026 \\
\bottomrule
\end{tabular}
\end{table}

The physical system confirms the synthetic results: the grade hierarchy holds under real gravitational dynamics.
Full architectures (CliffordNet, HC-Net) achieve 100\% on both tasks by having access to all grades simultaneously, but require $70\text{--}150\times$ more parameters than the 4.4K-parameter mean-field classifiers.

\subsection{Experiment 4: MD17 Force Prediction}
\label{sec:exp4}

\textbf{Setup.}
We benchmark on the MD17 molecular dynamics dataset~\cite{chmiela2017machine}, predicting atomic forces from positions across all 8 molecules.
Two models are trained: our energy-conserving HC-Net (forces derived via $\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E$, 866K params) and EGNN~\cite{satorras2021n} (398K params).
Published baselines (NequIP, MACE, PaiNN, SchNet) are cited at the standard 9500-sample split.
All results are reported in physical units (meV/\AA{}).

Training: 9500 train, 500 val, 1000 test, 100 epochs, 3 seeds.

\textbf{Results.}
\begin{table}[h]
\centering
\caption{MD17 force prediction: MAE (meV/\AA{}, lower is better). HC-Net outperforms EGNN on all molecules except benzene. Published SOTA methods (NequIP, MACE) use specialized spherical harmonic representations.}
\label{tab:md17}
\begin{tabular}{lcccccc}
\toprule
\textbf{Molecule} & \textbf{HC-Net} & \textbf{EGNN} & \textbf{NequIP}$^\dagger$ & \textbf{MACE}$^\dagger$ & \textbf{PaiNN}$^\dagger$ & \textbf{SchNet}$^\dagger$ \\
\midrule
Ethanol     & \textbf{36.3}  & 765.6  & 2.4  & 2.1  & 5.2  & 8.0  \\
Malonaldeh. & \textbf{83.8}  & 907.7  & 3.6  & 3.2  & 7.2  & 11.2 \\
Naphthalene & \textbf{44.5}  & 887.8  & 1.8  & 1.3  & 3.4  & 5.8  \\
Salicylic   & \textbf{62.3}  & 598.7  & 4.0  & 3.3  & 7.6  & 12.4 \\
Aspirin     & \textbf{64.1}  & 575.9  & 8.8  & 6.6  & 12.6 & 23.1 \\
Toluene     & \textbf{72.8}  & 719.1  & 1.6  & 1.2  & 3.0  & 5.5  \\
Uracil      & \textbf{108.6} & 277.9  & 3.1  & 2.6  & 5.6  & 9.5  \\
Benzene     & 572.4 & \textbf{631.8} & 0.3  & 0.3  & 0.8  & 1.7  \\
\bottomrule
\end{tabular}

\vspace{0.3em}
\small{$^\dagger$Published baselines at 9500 train~\cite{batzner2022nequip, batatia2022mace, schutt2021equivariant, schutt2017schnet}; not retrained.}
\end{table}

\textbf{Analysis.}
HC-Net outperforms EGNN by $5\text{--}21\times$ on 7 of 8 molecules, demonstrating the value of the hybrid local-global architecture.
Benzene is an exception: its high D$_{6h}$ symmetry produces near-zero forces, making force prediction degenerate (both models fail).

The gap to published SOTA (NequIP, MACE) is expected: these methods use spherical harmonic tensor products specifically designed for atomic environments, whereas HC-Net is a \emph{general} geometric architecture.
HC-Net's contribution is not absolute force prediction accuracy but rather the $\mathcal{O}(N)$ scaling advantage (Section~\ref{sec:exp5}) and the grade hierarchy insight that applies across domains.

\subsection{Experiment 5: Computational Scaling}
\label{sec:exp5}

\textbf{Setup.}
We measure forward pass time as particle count $N$ varies from 10 to 5000, fitting power-law complexity $\text{time} \propto N^\alpha$.
Four models are compared: HC-Net ($\mathcal{O}(kN)$ local + $\mathcal{O}(N)$ global), EGNN ($\mathcal{O}(N^2)$ pairwise), CliffordNet ($\mathcal{O}(N)$), and MLP baseline ($\mathcal{O}(N)$).

\textbf{Results.}
\begin{table}[h]
\centering
\caption{Scaling analysis: forward time (ms) and fitted complexity $\alpha$. EGNN runs out of memory (OOM) at $N \geq 500$. HC-Net scales to $N = 5000$ at $\mathcal{O}(N)$.}
\label{tab:scaling}
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{N=10} & \textbf{N=50} & \textbf{N=100} & \textbf{N=500} & \textbf{N=1K} & \textbf{N=2K} & \textbf{N=5K} & $\alpha$ \\
\midrule
HC-Net      & 4.7  & 7.4  & 5.7  & 11.1  & 12.5  & 15.2  & 28.8  & 0.27 \\
EGNN        & 3.2  & 5.3  & 16.2 & OOM   & OOM   & OOM   & OOM   & 1.31$^*$ \\
CliffordNet & 1.2  & 1.3  & 1.3  & 1.7   & 1.7   & 1.6   & 2.0   & 0.07 \\
MLP         & 0.3  & 0.3  & 0.3  & 0.3   & 0.3   & 0.3   & 0.3   & 0.01 \\
\bottomrule
\end{tabular}

\vspace{0.3em}
\small{$^*$Fitted from $N = 10, 50, 100$ only (OOM at $N \geq 500$ on 80GB A100).}
\end{table}

EGNN's $\mathcal{O}(N^2)$ pairwise message passing exhausts GPU memory at $N = 500$, while HC-Net scales comfortably to $N = 5000$ with only $2.4\times$ the time of CliffordNet.
This is the key practical advantage: for large molecular systems (proteins, materials), $\mathcal{O}(N)$ scaling is essential.

\subsection{Experiment 6: 3D SOTA Comparison}
\label{sec:exp6}

\textbf{Setup.}
We compare HC-Net against four equivariant architectures on 3D N-body trajectory prediction: CGENN~\cite{ruhe2023clifford}, EGNN~\cite{satorras2021n}, NequIP~\cite{batzner2022nequip}, and an MLP baseline with attention.
CGENN's hidden channels are set to produce comparable parameter counts ($\sim$400K vs.\ HC-Net's 324K).
Training sizes: 100, 500, 1000 samples; 100 epochs, 3 seeds.

\textbf{Results.}
\begin{table}[h]
\centering
\caption{3D N-body prediction: test MSE ($\times 10^{-6}$, lower is better). HC-Net achieves the best accuracy at small training sizes with the fewest parameters.}
\label{tab:sota}
\begin{tabular}{lrcccc}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{$n$=100} & \textbf{$n$=500} & \textbf{$n$=1000} \\
\midrule
\textbf{HC-Net}  & \textbf{324K} & \textbf{8 $\pm$ 0} & \textbf{2 $\pm$ 0} & 2 $\pm$ 0 \\
CGENN            & 401K          & 13 $\pm$ 1         & 3 $\pm$ 3          & \textbf{1 $\pm$ 0} \\
EGNN             & 566K          & 129 $\pm$ 37       & 18 $\pm$ 3         & 12 $\pm$ 1 \\
NequIP           & 728K          & 33 $\pm$ 8         & 8 $\pm$ 2          & 4 $\pm$ 1 \\
Baseline         & 399K          & 180 $\pm$ 29       & 24 $\pm$ 4         & 8 $\pm$ 3 \\
\bottomrule
\end{tabular}
\end{table}

HC-Net achieves the best accuracy at $n = 100$ and $n = 500$ with 19\% fewer parameters than CGENN.
At $n = 1000$, CGENN slightly edges ahead (MSE $1 \times 10^{-6}$ vs.\ $2 \times 10^{-6}$), suggesting that CGENN's exact equivariance provides a small advantage with sufficient data.
Both Clifford-algebra methods dramatically outperform EGNN and NequIP on this task.

\subsection{Experiment 7: Ablation Study}
\label{sec:exp7}

\textbf{Setup.}
We ablate HC-Net's components on MD17 ethanol force prediction (1000 train, 100 epochs, 3 seeds) by toggling individual components off while keeping all others intact.

\textbf{Component ablation results.}
\begin{table}[h]
\centering
\caption{Component ablation on MD17 ethanol. Removing local MPNN or residual connections is catastrophic ($>$550\% MSE increase). The full model achieves MSE = 0.141.}
\label{tab:ablation}
\begin{tabular}{lcccr}
\toprule
\textbf{Variant} & \textbf{Test MSE} & \textbf{Test MAE} & \textbf{$\Delta$ MSE} & \textbf{Params} \\
\midrule
Full             & 0.141 $\pm$ 0.013 & 0.277 $\pm$ 0.018 & ---          & 866K \\
$-$ Local MPNN   & 0.932 $\pm$ 0.031 & 0.710 $\pm$ 0.011 & +559\%       & 394K \\
$-$ Global MF    & 0.392 $\pm$ 0.415 & 0.397 $\pm$ 0.239 & +177\%       & 758K \\
$-$ Geo mixing   & 0.175 $\pm$ 0.030 & 0.307 $\pm$ 0.028 & +23\%        & 864K \\
$-$ Residual     & 1.016 $\pm$ 0.024 & 0.746 $\pm$ 0.008 & +618\%       & 866K \\
$-$ Layer norm   & 0.246 $\pm$ 0.023 & 0.371 $\pm$ 0.020 & +74\%        & 864K \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Architecture efficiency.}
Layer count and hidden dimension sweeps reveal the architecture is most efficient at smaller scales (Table~\ref{tab:arch_sweep}).

\begin{table}[h]
\centering
\caption{Architecture sweep on MD17 ethanol (single seed). The optimal configuration uses 2 layers and hidden dimension 64, achieving lower MSE than the default (4 layers, 128 hidden) with $2\times$ fewer parameters.}
\label{tab:arch_sweep}
\begin{tabular}{ccccr}
\toprule
\textbf{Layers} & \textbf{Hidden} & \textbf{Test MSE} & \textbf{Test MAE} & \textbf{Params} \\
\midrule
1  & 128 & 0.103 & 0.229 & 230K \\
\textbf{2}  & \textbf{128} & \textbf{0.076} & \textbf{0.196} & \textbf{442K} \\
4  & 128 & 0.158 & 0.299 & 866K \\
6  & 128 & 1.017 & 0.744 & 1.3M \\
8  & 128 & 1.038 & 0.753 & 1.7M \\
\midrule
4  & 32  & 0.187 & 0.322 & 68K \\
4  & \textbf{64}  & \textbf{0.120} & \textbf{0.251} & \textbf{233K} \\
4  & 128 & 0.158 & 0.299 & 866K \\
4  & 256 & 0.394 & 0.458 & 3.3M \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}
\begin{enumerate}[nosep]
    \item \textbf{Local MPNN is the most critical learned component} (+559\%). Without neighbor message passing, the model relies solely on global mean-field, which cannot capture precise interatomic interactions.
    \item \textbf{Residual connections are essential} (+618\%). Without skip connections, gradient flow degrades and the model essentially fails to train.
    \item \textbf{Global mean-field provides significant value} (+177\%), but with high variance across seeds---some runs still learn without it, others fail completely.
    \item \textbf{Geometric mixing is the least critical} (+23\%), suggesting the outer-product grade interactions provide a modest refinement rather than a core capability.
    \item \textbf{The architecture is most efficient with 2 layers} (MSE = 0.076 vs.\ 0.158 for 4 layers). Models with $\geq$6 layers fail to train (MSE $\approx$ 1.0), indicating gradient degradation despite residual connections.
    \item \textbf{Hidden dimension 64 outperforms 128 and 256}, suggesting that the 1000-sample training set cannot support larger models. The optimal model (2 layers, 128 hidden, 442K params) uses half the parameters of the default configuration.
\end{enumerate}

%==============================================================================
\section{Discussion}
%==============================================================================

\textbf{Grade hierarchy as a design principle.}
Our experiments establish that the choice of Clifford algebra grade determines what geometric information a mean-field can capture.
This is not a quantitative trade-off but a qualitative one: bivectors \emph{cannot} represent chirality regardless of classifier capacity, and vectors \emph{cannot} represent rotation direction.
This suggests a general principle for $\Cl(p,q)$: the pseudoscalar (highest grade) carries chirality information specific to that dimension.

\textbf{When to use HC-Net vs.\ specialized architectures.}
On MD17, HC-Net outperforms EGNN by $5\text{--}21\times$ but trails NequIP/MACE by $5\text{--}30\times$.
This gap reflects a fundamental design choice: NequIP and MACE use spherical harmonic tensor products specifically designed for atomic environments, while HC-Net is a \emph{general} geometric architecture.
HC-Net is best suited for domains where: (1) $\mathcal{O}(N)$ scaling matters (large particle systems), (2) multiple geometric grades are relevant (chirality-sensitive tasks), or (3) domain-specific representations are unavailable.

\textbf{Architecture efficiency.}
The ablation reveals that HC-Net is over-parameterized at the default 4 layers / 128 hidden for 1000-sample MD17.
The optimal 2-layer, hidden-64 configuration (233K params) achieves 24\% lower MSE than the 4-layer default (866K params), suggesting practitioners should start small and scale up only as training data increases.

\textbf{Limitations.}
The current architecture predicts forces directly; while an energy-conserving variant exists (Section~\ref{sec:exp4}), it does not close the gap to NequIP/MACE.
The $k$-NN graph construction assumes fixed molecular topology.
The grade hierarchy experiments use hand-crafted grade projections; learning optimal grade combinations remains open.

%==============================================================================
\section{Conclusion}
%==============================================================================

We established a strict grade hierarchy in Clifford algebras for geometric deep learning: vectors lose all geometric information under averaging ($\sim$50\% accuracy), bivectors preserve rotation (100\%) but not chirality ($\sim$50\%), and only trivectors preserve chirality (100\%).
This hierarchy, validated across 2D/3D synthetic, physical, and molecular tasks, provides a principled guide for choosing representations in equivariant architectures.

Our hybrid HC-Net architecture combines $k$-NN message passing with $\mathcal{O}(N)$ Clifford mean-field aggregation, achieving the best accuracy among five equivariant methods on 3D N-body prediction with 19\% fewer parameters.
On computational scaling, HC-Net runs to $N = 5000$ while EGNN exhausts GPU memory at $N = 500$.
Ablation reveals local message passing and residual connections as critical components, with optimal efficiency at 2 layers and 64 hidden dimensions.

\textbf{Future work.}
Extension to $\Cl(3,1)$ (Minkowski space) for relativistic systems.
Application to large molecular systems (proteins, materials) where $\mathcal{O}(N)$ scaling is essential.
Learning optimal grade combinations rather than hand-crafted projections.

%==============================================================================
\section*{Code Availability}
%==============================================================================

The source code for HC-Net is publicly available at \url{https://github.com/hpicsk/hc-net}.

%==============================================================================
% References
%==============================================================================

\bibliographystyle{plain}
\bibliography{references}

\end{document}
